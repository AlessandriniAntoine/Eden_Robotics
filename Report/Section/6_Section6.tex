\section{JoystickControl}\insertloftspace
\setcounter{figure}{0}\setcounter{table}{0}

\subsection{Principle}

\hspace{\parindent} The initial request of our customer was a remote controlled arm. This was the easiest and fastest solution to implement. Mr. Kedziora also wanted to avoid using artificial intelligence in order not to lengthen the prototype manufacturing time.

\bigbreak
In this configuration, a camera, fixed on the arm at the level of the hand, allows the user to have a video return. An Xbox controller in our case is used to control the arm. The commands sent by the user are done in the camera's frame.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\textwidth]{images/Section06/camera.png}
    \caption{Camera position}
    \label{fig:mesh16}
\end{figure}
\FloatBarrier

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\textwidth]{images/Section06/camera\_view.png}
    \caption{Camera view}
    \label{fig:mesh17}
\end{figure}
\FloatBarrier

\bigbreak
As explained in the previous section, the inverse kinematics calculates the necessary angles for a given position in the base frame \{s\}. However, it seemed more natural to us to make the user work in the camera reference frame \{c\} and then to change the reference frame. For this, we reused the modern robotics library. In the same way that we can calculate the transformation matrix between the base and the end effector from the position of the links, we can calculate the one between the base and the camera. 
\begin{center}
    $T_{sc} =\displaystyle \prod_{n=1}^4e^{[S_i]\theta_i}M_c$
\end{center}

\bigbreak
Then the relation between the position in the reference frame and the position in the reference frame is the following:
\begin{center}
    $p_s = T_{sc}p_c$
\end{center}

\bigbreak
Attention, $T_sc$ being a 4x4 matrix, $p_s$ and $p_c$ are homogeneous positions:$p = [p\hspace{0.2cm}1]$. At each command sent by the user, we retrieve the position of each of the links and then we apply the following code that puts into practice the previous equations. We obtain the position in the base frame.

\bigbreak
\begin{minted}[linenos=true,bgcolor=LightYellow]{Python}
    # import kinematics parameters
    from parameters import * 
    import modern_robotics as mr
    # receive angles and desired position from ros 
    thetalist = get_angles()
    p_c = get_position()
    p_c = np.array([p_c 1])
    # get transformation matrix and change the position
    t = mr.FKinSpace(m_c,screw_list,thetalist)
    p_s = np.dot(t,p_c)
    p_s = p[:3]
\end{minted}

\bigbreak
The control of the three axes is done with the two joysticks on the joystick. The left joystick allows to move according to the plane (x,y) of the camera and the joystick manages the depth. Some features listed below have also been added on the buttons to facilitate the work of the user and the proper functioning of the arm.
\begin{itemize}[noitemsep]
    \item authorize the movement: button A
    \item prohibit the movement: button B
    \item return to zero position : button X
    \item return to deposit position : button Y
\end{itemize}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\textwidth]{images/Section06/xbox\_controller.png}
    \caption{Xbox Controller}
    \label{fig:mesh18}
\end{figure}
\FloatBarrier

\subsection{Open loop}

\hspace{\parindent} In this section, we only present the operation and results of the open loop using the tools explained above. The implementation of each block as well as the information exchanges are done with the help of ROS. Everything is detailed in the dedicated part.

\bigbreak
The user sends a desired position with the joystick, a change of reference frame is performed and then the necessary angles are calculated. The diagram above explains the different blocks used and the information exchanged between them. The camera block is separate because it does not really intervene in the loop. It is just used to obtain a video return.
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\textwidth]{images/Section06/openloop\_joystick.png}
    \caption{Open loop for joystick control}
    \label{fig:mesh19}
\end{figure}
\FloatBarrier

\bigbreak 
To test the open loop, we subjected our system to steps along the three camera axes. We measured the error, response time and overshoot. The results are detailed below. All the results presented were obtained in simulation. We did not buy a sensor to measure the position of the real robot. We have therefore based ourselves on the simulation results and on the observations of the arm movements to validate the control.

\bigbreak
Using the joystick, the user gives an approximate position and corrects the desired position thanks to the camera feedback. As the results show, the open loop is accurate and allows to follow a position. Thus, it was not necessary to set up a closed loop on the real robot. We thus avoided buying a position sensor and saved development and delivery time.

\subsection{Closed loop}


\hspace{\parindent} However, we still servoed the robot in position in simulation. To stick to the reality, we have modified a little the robot in simulation compared to the reference robot used. Thus, we were closer to a real case and we were able to test a positional servoing using a proportional integral corrector.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\textwidth]{images/Section06/closedloop\_joystick.png}
    \caption{Closed loop for joystick control}
    \label{fig:mesh20}
\end{figure}
\FloatBarrier